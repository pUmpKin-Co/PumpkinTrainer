run_name: Demo
seed: 322
run_strategy: epoch
run_duration: 1
dry_run: false
model: {}
optimizer:
  name: adamw
  learning_rate: 0.0002
  weight_decay: 0.0
  betas:
  - 0.9
  - 0.95
  decay_norm_and_bias: false
  decay_embeddings: false
  metrics_log_interval: null
deepspeed:
  enabled: false
  stage: 0
  offload_optimizer: false
  offload_param: false
scheduler:
  name: cosine_annealing
  units: by_steps
  t_warmup: 100
  t_method: linear
  t_factor: 0.1
  t_min: 1.0e-06
  grad_clip_warmup_steps: null
  grad_clip_warmup_factor: null
data:
  paths: null
  num_workers: 0
  drop_last: false
  pin_memory: false
  prefetch_factor: null
  persistent_workers: false
  timeout: 0
restore_dataloader: true
fast_forward_batches: null
evaluators: []
eval_interval: 1000
save_folder: ./
save_interval: 1000
save_interval_unsharded: null
save_interval_ephemeral: null
save_num_checkpoints_to_keep: 1
save_num_unsharded_checkpoints_to_keep: -1
save_overwrite: false
force_save_unsharded: false
load_path: null
load_path_sharded_checkpointer: null
reset_optimizer_state: false
reset_trainer_state: false
sharded_checkpointer: torch_legacy
device_train_batch_size: 32
device_eval_batch_size: 32
max_grad_norm: 1.0
precision: amp_fp16
wandb:
  enabled: false
  project: null
  entity: CustomTrainer
  group: null
  name: null
  tags:
  - watching
  log_artifacts: false
  rank_zero_only: true
  log_interval: 1
speed_monitor:
  window_size: 100
  gpu_flops_available: null
console_log_interval: 1
compile: null
fsdp:
  enabled: false
  use_orig_params: true
  sharding_strategy: FULL_SHARD
  wrapping_strategy: null
  precision: pure
time_limit: 171000.0
early_stopping_factor: null
save_data_indices: true
python_profiling: false
torch_profiling: false
stop_at: null
activation_checkpointing: null
gradient_accumulation_steps: 1
