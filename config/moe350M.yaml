run_name: "MoE"
run_strategy: "epoch"
run_duration: 1
seed: 322
dry_run: False
save_folder: "/home/aiscuser/SparseTrainingOutput"
device_train_batch_size: 32
device_eval_batch_size: 32
max_grad_norm: 1.0
precision: amp_bf16
gradient_accumulation_steps: 1
activation_checkpointing: True
accelerator_type: "gpu"
console_log_interval: 100

checkpoint:
  save_num_checkpoints_to_keep: 1
  save_interval: 1000
  save_strategy: "step"

optimizer:
  name: "adamw"
  learning_rate: 0.001
  weight_decay: 0.1
  betas: [ 0.9, 0.95 ]

scheduler:
  name: "cosine_annealing"
  t_warmup: 2000
  t_min: 0.000001

wandb:
  enabled: False
  project: "sparse_training"
  group: "MoE_Debug"
  name: "debug_run"
  log_interval: 10
  

deepspeed:
  enabled: True
  stage: 2

fsdp:
  enabled: False

